# Attention-Modules-in-ConvNextV2
Implementation of Attention Modules in ConvNext V2. SE, ECA, CBAM, Triplet Attention.
This implementation is based on MMPtrain Framework from OpenMMLab.
mmdet                     3.3.0                     
mmengine                  0.10.6                   
mmpretrain                1.2.0

Modules implemented:
- SE (Squeeze-and-Excitation): Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-excitation networks.
 In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
 7132–7141.
- ECA (Efficient Channel Attention):  Wang, Q., Wu, B., Zhu, P., Li, P., Zuo, W., and Hu, Q. (2020). Eca-net: Efficient channel attention for deep convolutional neural networks. In Proceedings of the IEEE/CVF
 conference on computer vision and pattern recognition, pages 11534– 115
- CBAM (Convolutional Block Attention Module): Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). Cbam: Convolutional block attention module. In Proceedings of the European conference on computer vision (ECCV), pages 3
19.
- Triplet Attention: Misra, D., Nalamada, T., Arasanipalai, A. U., and Hou, Q. (2021). Rotate to attend: Convolutional triplet attention module. In Proceedings of the IEEE/CVF winter conference on
 applications of computer vision, pages 3139–3148.

